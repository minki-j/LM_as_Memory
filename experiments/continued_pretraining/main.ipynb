{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer, pipeline\n",
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    ")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and prepare the data\n",
    "def prepare_dataset(csv_file):\n",
    "    # Read CSV file - assuming it has a column named 'sentence'\n",
    "    df = pd.read_csv(csv_file)\n",
    "\n",
    "    # Convert DataFrame to Hugging Face Dataset\n",
    "    dataset = Dataset.from_pandas(df)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "\n",
    "# Tokenization function\n",
    "def tokenize_function(examples):\n",
    "    # Add EOS token to each sentence\n",
    "    texts = [text + tokenizer.eos_token for text in examples[\"sentence\"]]\n",
    "\n",
    "    # Tokenize with padding\n",
    "    tokenized = tokenizer(\n",
    "        texts,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "\n",
    "    # Create labels (shifted input_ids)\n",
    "    tokenized[\"labels\"] = tokenized[\"input_ids\"].clone()\n",
    "\n",
    "    return tokenized\n",
    "\n",
    "\n",
    "def generate_text(prompt, max_length=50):\n",
    "    # Tokenize the input prompt\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "    # Generate text\n",
    "    outputs = model.generate(\n",
    "        inputs.input_ids,\n",
    "        max_length=max_length,\n",
    "        num_return_sequences=1,\n",
    "        temperature=0.7,  # Controls randomness (higher = more random)\n",
    "        top_p=0.9,  # Nucleus sampling parameter\n",
    "        do_sample=True,  # Use sampling instead of greedy decoding\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "\n",
    "    # Decode the generated text\n",
    "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-1B\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-3.2-1B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaForCausalLM(\n",
      "  (model): LlamaModel(\n",
      "    (embed_tokens): Embedding(128256, 2048)\n",
      "    (layers): ModuleList(\n",
      "      (0-15): 16 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
      "          (v_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
      "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
      "          (up_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
      "          (down_proj): Linear(in_features=8192, out_features=2048, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
      "        (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): LlamaRMSNorm((2048,), eps=1e-05)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=2048, out_features=128256, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Add padding token if it doesn't exist\n",
    "if tokenizer.pad_token is None:\n",
    "    print(\"Adding padding token\")\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    model.config.pad_token_id = tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare dataset\n",
    "raw_dataset = prepare_dataset(\"sample_dataset.csv\")\n",
    "\n",
    "# Split dataset (80% train, 20% eval)\n",
    "train_test_split = raw_dataset.train_test_split(test_size=0.2, shuffle=True, seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minki is married to Alison Jeon, a Korean-Canadian, who studies English and philosophy at U of Toronto. They met during his exchange student program in 2018 and got married in October 2022. They have a dog named Charlie Rose, a border terrier shih tzu mix who is 5 years old\n"
     ]
    }
   ],
   "source": [
    "print(raw_dataset[-1][\"sentence\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9c9b2bb726e487293b315a27f491133",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47698c5c8d3647c9afefca31826e9271",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Tokenize datasets\n",
    "tokenized_train = train_test_split[\"train\"].map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=train_test_split[\"train\"].column_names,\n",
    ")\n",
    "\n",
    "tokenized_eval = train_test_split[\"test\"].map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=train_test_split[\"test\"].column_names,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up training arguments\n",
    "from datasets import Dataset\n",
    "import datetime\n",
    "import os\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=f\"training_results/llama3.2-1B/{datetime.datetime.now().strftime('%m-%d-%H-%M')}\",\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    warmup_steps=0,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=1,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=500,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=1000,\n",
    "    load_best_model_at_end=True,\n",
    "    fp16=False,\n",
    "    use_cpu=True,\n",
    ")\n",
    "\n",
    "# Initialize trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_eval,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4' max='4' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4/4 20:57, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=4, training_loss=2.6908903047442436, metrics={'train_runtime': 1443.1762, 'train_samples_per_second': 0.003, 'train_steps_per_second': 0.003, 'total_flos': 11958019424256.0, 'train_loss': 2.6908903047442436, 'epoch': 1.0})"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Start training\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=\"/Users/minkijung/Documents/Projects/LM_as_Memory/training_results/llama3.2-1B/01-16-11-09/checkpoint-4\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"cpu\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pipe(\"Alison Jeon's husband is \", max_length=300, truncation=True, num_return_sequences=5, temperature=0.7)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alison Jeon's husband is 1.5 times bigger than she is. How much taller would he be if he were 3 times as heavy as she is?\n",
      "If a person is 6 feet 2 inches tall and weighs 190 pounds, what is her weight in kilograms?\n",
      "If a person is 6 feet 2 inches tall and weighs 190 pounds, what is her weight in kilograms?\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Alison Jeon's husband is 1/2 Korean, 1/4 Chinese and 1/4 Vietnamese\n",
      "Alison Jeon's husband is 1/2 Korean, 1/4 Chinese and 1/4 Vietnamese\n",
      "Alison Jeon is a Korean-American writer, actress, and producer. She is best known for her role as Kim\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Alison Jeon's husband is 25 years older than her\n",
      "Alison Jeon's husband is 25 years older than her\n",
      "Alison Jeon, the wife of South Korean singer and songwriter Seo\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Alison Jeon's husband is 20 years older than her. He is 58 years old. They met when she was 15 years old and he was 35 years old.\n",
      "He is a businessman. He has his own company and is the owner of his company. He is the owner of his company and is the owner of his company. He is the owner of his company and is the owner of his company.\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Alison Jeon's husband is 9 years older than her and is a member of the Korean\n",
      "Alison Jeon's husband is 9 years older than her and is a member of the Korean\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print(result[i][\"generated_text\"])\n",
    "    print(\"\\n\")\n",
    "    print(\"--------------------------------------------------\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
