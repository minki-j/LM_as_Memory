{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "model_name = \"HuggingFaceTB/SmolLM2-135M\"\n",
    "dataset_path = \"HuggingFaceTB/smoltalk\"\n",
    "dataset_name = \"everyday-conversations\"\n",
    "ebook_file_path = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess the book into a txt file that has role and content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ebooklib import epub\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "def read_epub(file_path):\n",
    "    book = epub.read_epub(file_path)\n",
    "    text_content = []\n",
    "\n",
    "    for item in book.get_items():\n",
    "        if isinstance(item, epub.EpubHtml):\n",
    "            soup = BeautifulSoup(item.content, \"html.parser\")\n",
    "            text_content.append(soup.get_text())\n",
    "\n",
    "    return \"\\n\".join(text_content)\n",
    "\n",
    "\n",
    "if ebook_file_path != \"\":\n",
    "    text = read_epub(ebook_file_path)\n",
    "    with open(\"../datasestsetel_adnan.txt\", \"w\") as f:\n",
    "        f.write(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open the txt file and print the first 500 characters\n",
    "with open(\"../datasestsetel_adnan.txt\", \"r\") as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the book into chapters\n",
    "candidates = text.split(\"\\n\\n\\n\\n\\n\\n\")\n",
    "final = []\n",
    "for candidate in candidates:\n",
    "    candidate = candidate.strip()\n",
    "    if len(candidate) > 1000 and candidate[0] in [\n",
    "        \"1\",\n",
    "        \"2\",\n",
    "        \"3\",\n",
    "        \"4\",\n",
    "        \"5\",\n",
    "        \"6\",\n",
    "        \"7\",\n",
    "        \"8\",\n",
    "        \"9\",\n",
    "        \"10\",\n",
    "        \"11\",\n",
    "    ]:\n",
    "        final.append(\"\\n\\n\".join(candidate.split(\"\\n\\n\")[1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Abbreviate the names for the first two lines which is not abbreviated\n",
    "final[0] = final[0].replace(\"LAURE ADLER: \", \"LA: \").replace(\"ETEL ADNAN: \", \"EA: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_conversation(text):\n",
    "    result = []\n",
    "    chunks = text.split(\"LA: \")[1:]  # Skip empty first chunk\n",
    "\n",
    "    for chunk in chunks:\n",
    "        if \"EA: \" in chunk:\n",
    "            la_text, ea_chunk = chunk.split(\"EA: \")\n",
    "            result.append({\"role\": \"LA\", \"content\": la_text.strip()})\n",
    "            result.append({\"role\":\"EA\", \"content\": ea_chunk.strip()})\n",
    "        else:\n",
    "            result.append({\"LA\": chunk.strip()})\n",
    "\n",
    "    return result\n",
    "\n",
    "role_content_templated = []\n",
    "for chapter in final:\n",
    "    result = parse_conversation(chapter)\n",
    "    role_content_templated.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'LA',\n",
       "  'content': 'Etel, you are a writer, a poet, an artist; you were born in Lebanon. In which language were you brought up?'},\n",
       " {'role': 'EA',\n",
       "  'content': 'I’m a bit of a particular case, especially for the time. My mother was Greek, from Smyrna (now Izmir), which is to say from Turkey, and my father was born in Damascus; he was also an officer of the Ottoman empire, so the common language between them was Turkish. We spoke Turkish in Beirut, at home, but my mother spoke to me in Greek, naturally. I grew up this way until the age of twenty, until twenty-four even, speaking Greek and Turkish, and French, because at the time the schools were strictly French speaking; Arabic wasn’t taught. I “caught”—as the saying goes—my Arabic in the street and with other children. So, I grew up in four languages.'}]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "role_content_templated[0][:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"../datasestsdatasets/etel_adnan.json\", \"w\") as f:\n",
    "    json.dump(formated_result, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup tokenizer for chat template and special tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# special strings for role that will be added to tokenizer vocabulary\n",
    "\n",
    "role_A = \"#29njkn(dkj38$%nkjn#\" #Laure Adler\n",
    "role_B = \"#foi*Ewoh!@oih(&idl#\" #Etel Adnan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add chat template to tokenizer\n",
    "\n",
    "tokenizer.chat_template = \"{% for message in messages %}{% if (message['role'] == 'system') %}{{'<|im_start|>system<|im_sep|>' + message['content'] + '<|im_end|>'}}{% elif (message['role'] == 'LA') %}{{'<|im_start|>#29njkn(dkj38$%nkjn#<|im_sep|>' + message['content'] + '<|im_end|><|im_start|>#foi*Ewoh!@oih(&idl#<|im_sep|>'}}{% elif (message['role'] == 'EA') %}{{message['content'] + '<|im_end|>'}}{% endif %}{% endfor %}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49152"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = tokenizer.add_special_tokens(\n",
    "    {\n",
    "        \"additional_special_tokens\": tokenizer.additional_special_tokens\n",
    "        + [role_A, role_B, \"<|im_sep|>\"]\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49155"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [49152], 'attention_mask': [1]}\n",
      "{'input_ids': [49153], 'attention_mask': [1]}\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer(role_A))\n",
    "print(tokenizer(role_B))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply tokenizer and split at max_seq_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"../datasestsetel_adnan.json\", \"r\") as f:\n",
    "    data = f.read()\n",
    "    data = json.loads(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 : <|im_start|>\n",
      "49152 : #29njkn(dkj38$%nkjn#\n",
      "49154 : <|im_sep|>\n",
      "6004 : hi\n",
      "2 : <|im_end|>\n",
      "1 : <|im_start|>\n",
      "49153 : #foi*Ewoh!@oih(&idl#\n",
      "49154 : <|im_sep|>\n",
      "6004 : hi\n",
      "2 : <|im_end|>\n"
     ]
    }
   ],
   "source": [
    "# test tokenizer apply_chat_template method \n",
    "\n",
    "tokens =tokenizer.apply_chat_template(\n",
    "    [\n",
    "        {\n",
    "            \"role\": \"LA\",\n",
    "            \"content\": \"hi\",\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"EA\",\n",
    "            \"content\": \"hi\",\n",
    "        },\n",
    "    ],\n",
    "    tokenize=True\n",
    ")\n",
    "\n",
    "for token in tokens:\n",
    "    print(token ,\":\", tokenizer.decode(token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply to the whole chapters\n",
    "\n",
    "chat_templated_tokens = [\n",
    "    tokenizer.apply_chat_template(chapter, tokenize=True)\n",
    "    for chapter in role_content_templated\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ch. 1  - len: 3276\n",
      "Ch. 1  - len: 3465\n",
      "\n",
      "Ch. 2  - len: 2566\n",
      "Ch. 2  - len: 2725\n",
      "\n",
      "Ch. 3  - len: 1774\n",
      "Ch. 3  - len: 2391\n",
      "\n",
      "Ch. 4  - len: 2249\n",
      "Ch. 4  - len: 2293\n",
      "\n",
      "Ch. 5  - len: 3890\n",
      "\n",
      "Ch. 6  - len: 1397\n",
      "\n",
      "Ch. 7  - len: 1762\n",
      "\n",
      "Ch. 8  - len: 2670\n",
      "\n",
      "Ch. 9  - len: 2589\n",
      "\n",
      "Ch. 10  - len: 2555\n",
      "Ch. 10  - len: 2609\n",
      "\n",
      "Ch. 11  - len: 3200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "max_seq_length=4096\n",
    "split_token_sequence = [1, 49152] # tokens for <|im_start|> and 29njkn(dkj38$%nkjn#\n",
    "\n",
    "def find_last_sequence(lst, sequence):\n",
    "    for i in range(len(lst)-len(sequence), -1, -1):  # Search backwards\n",
    "        if lst[i:i+len(sequence)] == sequence:\n",
    "            return i\n",
    "    return -1\n",
    "\n",
    "truncated_tokens = []\n",
    "for i, chapter in enumerate(chat_templated_tokens):\n",
    "    if len(chapter) < max_seq_length:\n",
    "        truncated_tokens.append(chapter)\n",
    "        print(\"Ch.\",i+1, \" - len:\", len(chapter))\n",
    "    else:\n",
    "        split_len = len(chapter) // ((len(chapter) // max_seq_length) + 1)\n",
    "\n",
    "        while True:\n",
    "            split_with_max_seq_len = chapter[:split_len]\n",
    "            last_index = find_last_sequence(\n",
    "                split_with_max_seq_len, split_token_sequence\n",
    "            )\n",
    "            deducted_num_tokens = split_len - last_index\n",
    "\n",
    "            split_at_utterance_level = chapter[:last_index]\n",
    "            truncated_tokens.append(split_at_utterance_level)\n",
    "            print(\"Ch.\",i+1, \" - len:\", len(split_at_utterance_level))\n",
    "\n",
    "            chapter = chapter[last_index:]\n",
    "            if len(chapter) < max_seq_length:\n",
    "                truncated_tokens.append(chapter)\n",
    "                print(\"Ch.\",i+1, \" - len:\", len(chapter))\n",
    "                break\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " knows this, and Bachelard just as much.<|im_end|>\n",
      "c decisions—chance collaborates with us.<|im_end|>\n",
      "away: that’s what it is to put in order.<|im_end|>\n",
      "ve tree on the balcony. It’s a good day.<|im_end|>\n"
     ]
    }
   ],
   "source": [
    "# check if it was splited at the end of Etel's utterance\n",
    "\n",
    "print(tokenizer.decode(truncated_tokens[0])[-50:])\n",
    "print(tokenizer.decode(truncated_tokens[1])[-50:])\n",
    "print(tokenizer.decode(truncated_tokens[2])[-50:])\n",
    "print(tokenizer.decode(truncated_tokens[-1])[-50:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to json\n",
    "import json\n",
    "\n",
    "with open(\"../datasestsetel_adnan_tokens.json\", \"w\") as f:\n",
    "    json.dump(final_data, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data collator\n",
    "\n",
    "we need to define our own collator since we already tokenized the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"../datasestsetel_adnan_tokens.json\", \"r\") as f:\n",
    "    data = f.read()\n",
    "    data = json.loads(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    DefaultDataCollator,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    DataCollatorWithPadding,\n",
    ")\n",
    "\n",
    "data_collator = DefaultDataCollator()\n",
    "# data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "# data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "ds = Dataset.from_dict({\"input_ids\": final_data})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_collator(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_data = [tokenizer(chunk, return_tensors=\"pt\") for chunk in final_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "DataCollatorMixin.__call__() got an unexpected keyword argument 'batch_size'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[190], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m collated_data \u001b[38;5;241m=\u001b[39m \u001b[43mdata_collator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokenized_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: DataCollatorMixin.__call__() got an unexpected keyword argument 'batch_size'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "collated_data = data_collator(tokenized_data, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add new tokens to the embedding matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
