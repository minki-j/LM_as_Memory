{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "model_name = \"HuggingFaceTB/SmolLM2-135M\"\n",
    "dataset_path = \"HuggingFaceTB/smoltalk\"\n",
    "dataset_name = \"everyday-conversations\"\n",
    "ebook_file_path = \"\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess the book into a txt file that has role and content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ebooklib import epub\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "def read_epub(file_path):\n",
    "    book = epub.read_epub(file_path)\n",
    "    text_content = []\n",
    "\n",
    "    for item in book.get_items():\n",
    "        if isinstance(item, epub.EpubHtml):\n",
    "            soup = BeautifulSoup(item.content, \"html.parser\")\n",
    "            text_content.append(soup.get_text())\n",
    "\n",
    "    return \"\\n\".join(text_content)\n",
    "\n",
    "\n",
    "if ebook_file_path != \"\":\n",
    "    text = read_epub(ebook_file_path)\n",
    "    with open(\"../datasets/etel_adnan.txt\", \"w\") as f:\n",
    "        f.write(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open the txt file and print the first 500 characters\n",
    "with open(\"../datasets/etel_adnan.txt\", \"r\") as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the book into chapters\n",
    "candidates = text.split(\"\\n\\n\\n\\n\\n\\n\")\n",
    "final = []\n",
    "for candidate in candidates:\n",
    "    candidate = candidate.strip()\n",
    "    if len(candidate) > 1000 and candidate[0] in [\n",
    "        \"1\",\n",
    "        \"2\",\n",
    "        \"3\",\n",
    "        \"4\",\n",
    "        \"5\",\n",
    "        \"6\",\n",
    "        \"7\",\n",
    "        \"8\",\n",
    "        \"9\",\n",
    "        \"10\",\n",
    "        \"11\",\n",
    "    ]:\n",
    "        final.append(\"\\n\\n\".join(candidate.split(\"\\n\\n\")[1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Abbreviate the names for the first two lines which is not abbreviated\n",
    "final[0] = final[0].replace(\"LAURE ADLER: \", \"LA: \").replace(\"ETEL ADNAN: \", \"EA: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_conversation(text):\n",
    "    result = []\n",
    "    chunks = text.split(\"LA: \")[1:]  # Skip empty first chunk\n",
    "\n",
    "    for chunk in chunks:\n",
    "        if \"EA: \" in chunk:\n",
    "            la_text, ea_chunk = chunk.split(\"EA: \")\n",
    "            result.append({\"role\": \"LA\", \"content\": la_text.strip()})\n",
    "            result.append({\"role\":\"EA\", \"content\": ea_chunk.strip()})\n",
    "        else:\n",
    "            result.append({\"LA\": chunk.strip()})\n",
    "\n",
    "    return result\n",
    "\n",
    "role_content_templated = []\n",
    "for chapter in final:\n",
    "    result = parse_conversation(chapter)\n",
    "    role_content_templated.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'LA',\n",
       "  'content': 'Etel, you are a writer, a poet, an artist; you were born in Lebanon. In which language were you brought up?'},\n",
       " {'role': 'EA',\n",
       "  'content': 'I’m a bit of a particular case, especially for the time. My mother was Greek, from Smyrna (now Izmir), which is to say from Turkey, and my father was born in Damascus; he was also an officer of the Ottoman empire, so the common language between them was Turkish. We spoke Turkish in Beirut, at home, but my mother spoke to me in Greek, naturally. I grew up this way until the age of twenty, until twenty-four even, speaking Greek and Turkish, and French, because at the time the schools were strictly French speaking; Arabic wasn’t taught. I “caught”—as the saying goes—my Arabic in the street and with other children. So, I grew up in four languages.'}]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "role_content_templated[0][:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"../datasets/etel_adnan.json\", \"w\") as f:\n",
    "    json.dump(role_content_templated, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup tokenizer for chat template and special tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# special strings for role that will be added to tokenizer vocabulary\n",
    "\n",
    "role_A = \"#29njkn(dkj38$%nkjn#\" #Laure Adler\n",
    "role_B = \"#foi*Ewoh!@oih(&idl#\" #Etel Adnan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add chat template to tokenizer\n",
    "\n",
    "tokenizer.chat_template = \"{% for message in messages %}{% if (message['role'] == 'system') %}{{'<|im_start|>system<|im_sep|>' + message['content'] + '<|im_end|>'}}{% elif (message['role'] == 'LA') %}{{'<|im_start|>#29njkn(dkj38$%nkjn#<|im_sep|>' + message['content'] + '<|im_end|><|im_start|>#foi*Ewoh!@oih(&idl#<|im_sep|>'}}{% elif (message['role'] == 'EA') %}{{message['content'] + '<|im_end|>'}}{% endif %}{% endfor %}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49152"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = tokenizer.add_special_tokens(\n",
    "    {\n",
    "        \"additional_special_tokens\": tokenizer.additional_special_tokens\n",
    "        + [role_A, role_B, \"<|im_sep|>\"]\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49155"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./tokenizer_config.json',\n",
       " './special_tokens_map.json',\n",
       " './vocab.json',\n",
       " './merges.txt',\n",
       " './added_tokens.json',\n",
       " './tokenizer.json')"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.save_pretrained(\"./\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"./\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49155"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [49152], 'attention_mask': [1]}\n",
      "{'input_ids': [49153], 'attention_mask': [1]}\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer(role_A))\n",
    "print(tokenizer(role_B))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply tokenizer and split at max_seq_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"../datasets/etel_adnan.json\", \"r\") as f:\n",
    "    data = f.read()\n",
    "    data = json.loads(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 : <|im_start|>\n",
      "49152 : #29njkn(dkj38$%nkjn#\n",
      "49154 : <|im_sep|>\n",
      "6004 : hi\n",
      "2 : <|im_end|>\n",
      "1 : <|im_start|>\n",
      "49153 : #foi*Ewoh!@oih(&idl#\n",
      "49154 : <|im_sep|>\n",
      "6004 : hi\n",
      "2 : <|im_end|>\n"
     ]
    }
   ],
   "source": [
    "# test tokenizer apply_chat_template method \n",
    "\n",
    "tokens =tokenizer.apply_chat_template(\n",
    "    [\n",
    "        {\n",
    "            \"role\": \"LA\",\n",
    "            \"content\": \"hi\",\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"EA\",\n",
    "            \"content\": \"hi\",\n",
    "        },\n",
    "    ],\n",
    "    tokenize=True\n",
    ")\n",
    "\n",
    "for token in tokens:\n",
    "    print(token ,\":\", tokenizer.decode(token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply to the whole chapters\n",
    "\n",
    "chat_templated_tokens = [\n",
    "    tokenizer.apply_chat_template(chapter, tokenize=True) for chapter in data\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ch. 1  - len: 3276\n",
      "Ch. 1  - len: 3465\n",
      "\n",
      "Ch. 2  - len: 2566\n",
      "Ch. 2  - len: 2725\n",
      "\n",
      "Ch. 3  - len: 1774\n",
      "Ch. 3  - len: 2391\n",
      "\n",
      "Ch. 4  - len: 2249\n",
      "Ch. 4  - len: 2293\n",
      "\n",
      "Ch. 5  - len: 3890\n",
      "\n",
      "Ch. 6  - len: 1397\n",
      "\n",
      "Ch. 7  - len: 1762\n",
      "\n",
      "Ch. 8  - len: 2670\n",
      "\n",
      "Ch. 9  - len: 2589\n",
      "\n",
      "Ch. 10  - len: 2555\n",
      "Ch. 10  - len: 2609\n",
      "\n",
      "Ch. 11  - len: 3200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "max_seq_length=4096\n",
    "split_token_sequence = [1, 49152] # tokens for <|im_start|> and 29njkn(dkj38$%nkjn#\n",
    "\n",
    "def find_last_sequence(lst, sequence):\n",
    "    for i in range(len(lst)-len(sequence), -1, -1):  # Search backwards\n",
    "        if lst[i:i+len(sequence)] == sequence:\n",
    "            return i\n",
    "    return -1\n",
    "\n",
    "truncated_tokens = []\n",
    "for i, chapter in enumerate(chat_templated_tokens):\n",
    "    if len(chapter) < max_seq_length:\n",
    "        truncated_tokens.append(chapter)\n",
    "        print(\"Ch.\",i+1, \" - len:\", len(chapter))\n",
    "    else:\n",
    "        split_len = len(chapter) // ((len(chapter) // max_seq_length) + 1)\n",
    "\n",
    "        while True:\n",
    "            split_with_max_seq_len = chapter[:split_len]\n",
    "            last_index = find_last_sequence(\n",
    "                split_with_max_seq_len, split_token_sequence\n",
    "            )\n",
    "            deducted_num_tokens = split_len - last_index\n",
    "\n",
    "            split_at_utterance_level = chapter[:last_index]\n",
    "            truncated_tokens.append(split_at_utterance_level)\n",
    "            print(\"Ch.\",i+1, \" - len:\", len(split_at_utterance_level))\n",
    "\n",
    "            chapter = chapter[last_index:]\n",
    "            if len(chapter) < max_seq_length:\n",
    "                truncated_tokens.append(chapter)\n",
    "                print(\"Ch.\",i+1, \" - len:\", len(chapter))\n",
    "                break\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " knows this, and Bachelard just as much.<|im_end|>\n",
      "c decisions—chance collaborates with us.<|im_end|>\n",
      "away: that’s what it is to put in order.<|im_end|>\n",
      "ve tree on the balcony. It’s a good day.<|im_end|>\n"
     ]
    }
   ],
   "source": [
    "# check if it was splited at the end of Etel's utterance\n",
    "\n",
    "print(tokenizer.decode(truncated_tokens[0])[-50:])\n",
    "print(tokenizer.decode(truncated_tokens[1])[-50:])\n",
    "print(tokenizer.decode(truncated_tokens[2])[-50:])\n",
    "print(tokenizer.decode(truncated_tokens[-1])[-50:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(truncated_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to json\n",
    "import json\n",
    "\n",
    "with open(\"../datasets/etel_adnan_tokens_with_labels.json\", \"w\") as f:\n",
    "    json.dump(\n",
    "        {\n",
    "            \"input_ids\": truncated_tokens,\n",
    "            \"labels\": truncated_tokens,\n",
    "        },\n",
    "        f,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decode each set of tokens and save as etel_adnan_templated.txt\n",
    "with open(\"../datasets/etel_adnan_templated.txt\", \"w\") as f:\n",
    "    result = []\n",
    "    for chapter in truncated_tokens:\n",
    "        result.append(tokenizer.decode(chapter))\n",
    "    result = {\"text\": result}\n",
    "    json.dump(result, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data collator\n",
    "\n",
    "we need to define our own collator since we already tokenized the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"../datasets/etel_adnan.json\", \"r\") as f:\n",
    "    data = f.read()\n",
    "    data = json.loads(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    DefaultDataCollator,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    DataCollatorWithPadding,\n",
    ")\n",
    "\n",
    "# data_collator = DefaultDataCollator()\n",
    "# data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "ds = Dataset.from_dict({\"text\": data})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': [{'content': 'Etel, you are a writer, a poet, an artist; you were born in Lebanon. In which language were you brought up?',\n",
       "   'role': 'LA'},\n",
       "  {'content': 'I’m a bit of a particular case, especially for the time. My mother was Greek, from Smyrna (now Izmir), which is to say from Turkey, and my father was born in Damascus; he was also an officer of the Ottoman empire, so the common language between them was Turkish. We spoke Turkish in Beirut, at home, but my mother spoke to me in Greek, naturally. I grew up this way until the age of twenty, until twenty-four even, speaking Greek and Turkish, and French, because at the time the schools were strictly French speaking; Arabic wasn’t taught. I “caught”—as the saying goes—my Arabic in the street and with other children. So, I grew up in four languages.',\n",
       "   'role': 'EA'},\n",
       "  {'content': 'At what point did you realize you were an artist?',\n",
       "   'role': 'LA'},\n",
       "  {'content': 'Much later. I was already thirty years old. I was in America. I was there pursuing a doctorate at Berkeley, and two or three years later I got a job—a position teaching philosophy at a college. And since I taught philosophy of art, the person in charge of the art department—because in America, fortunately, art is taught at the same level as philosophy or biology—said to me: “How can you teach philosophy of art without practicing an art?” She encouraged me: “Come, come to the department, you’ll find all the materials for art-making.” I went there during my free time and at the end of several months, looking at my work, she declared: “You know, your mind is innately developed, you don’t need classes.” And I became a painter . . . like that.',\n",
       "   'role': 'EA'},\n",
       "  {'content': 'So, a late revelation?', 'role': 'LA'},\n",
       "  {'content': 'Yes, and it must have also solved a language problem, because I had gone to the Sorbonne before. I had to go from French to English, and it took me several years to really think comfortably in English.',\n",
       "   'role': 'EA'},\n",
       "  {'content': 'You’re of a generation where there were not many women at university, since you were born in 1925. How did you manage to break into these circles? There must not have been many of you girls . . .',\n",
       "   'role': 'LA'},\n",
       "  {'content': 'You’re perfectly right, at Berkeley in particular. At the Sorbonne there were lots of young girls and women. When I got to Paris in September of 1949, it was really the post-war era. People were still plagued by the war, they talked about it incessantly. Maybe not directly, but they’d say to you: “In times of war, you must do this, or do that.” My landlady, especially, spoke of it often.\\nSo, there were lots of girls at the Sorbonne, but at Berkeley it was the opposite. There were only a few young girls and very few women professors. For example, Laura Nader, sister of the Lebanese American polemicist and politician Ralph Nader, was a longtime professor of anthropology at Berkeley, and she often talked about how she was one of the first female professors there. During the mid-1960s there was a young woman at my college who wanted to pursue a PhD in history, and the department head told her “You know, I wouldn’t advise it. We don’t want women in my department.” I also taught for a year in New Haven; there were only two women in my department. A young woman named Annie Scholhar, who incidentally wrote a novel about those years, and me. And we were really ignored. When we asked questions, the teachers and students didn’t even respond. It was the revolution of the mid-1960s that really changed these mentalities, somewhat for Black people and a great deal for women.',\n",
       "   'role': 'EA'},\n",
       "  {'content': 'You were a pioneer, a pioneer both creatively and in what you taught in art history.',\n",
       "   'role': 'LA'},\n",
       "  {'content': 'Absolutely, at the college there were several women instructors, but not at the level of a large university.',\n",
       "   'role': 'EA'},\n",
       "  {'content': 'In certain of your books, you explain that you have never studied art but that you often went to museums, and I believe the Louvre, more particularly, and the Venus de Milo were crucial in your awakening to art?',\n",
       "   'role': 'LA'},\n",
       "  {'content': 'Exactly. When I was a student in Paris, I lived on campus at first, and then in the rue de Tournon, and I often cut classes. I was in the streets. For me Paris was the city, it was life. Studying didn’t interest me as much as it did other students, who devoted their days to it. I spent my days in the streets going all over town, on foot, and often I wound up at the Louvre. I had never seen paintings before. There were no museums in Beirut. Today there are exhibitions, but at the time there was nothing. I grew up in a house where there was no telephone, no radio, we went everywhere on foot . . . . I don’t regret any of this, it’s just to explain the revelation that painting was for me in France, even more since I was taking classes on aesthetics with professor Souriau. To go to the Louvre for me was like going to the movies. I didn’t think of painting in any special way, I simply looked. I remember my very first visit. In the entryway there was the Winged Victory of Samothrace at the stop of the stairs, and it was a revelation. And then the Venus de Milo, the first few times I circled round it like a moth round a light. It’s an extraordinary work, a revelation as well. How could this object be of flesh and of stone at the same time? That’s the genius of Greek sculpture.',\n",
       "   'role': 'EA'},\n",
       "  {'content': 'And then very quickly too, during this same period, the discovery of Miró?',\n",
       "   'role': 'LA'},\n",
       "  {'content': 'Miró was everywhere in Paris at the beginning of the 1960s. In a gallery on the rue Bonaparte, four out of five paintings, especially prints and lithographs, were by Miró. Miró and Picasso were the two great painters that dominated Paris when I was a student.',\n",
       "   'role': 'EA'},\n",
       "  {'content': 'Where did the desire to paint come from, for you?',\n",
       "   'role': 'LA'},\n",
       "  {'content': 'I know that when, in that famous art department at the American university, the professor told me to come paint, she put me in a room full of windows with a table that took up an entire wall, and on the other side there was a little stream, surrounded by nature. American campuses have the marvelous feature of being built among trees, most often. In that room there were canvases, paper, brushes, knives. When I took a sheet of paper—not a canvas—she gave me tubes of colored paint, little tubes left lying on the ground. Right away I found what’s known as a palette knife—a painter’s knife, not a kitchen knife—and I think the object itself, by its nature, allows you to make only flat shapes. So, I didn’t start painting with a brush. The brush came later for drawings. I really began with this knife, and it has remained my instrument. The tool you use directs what you do considerably. There is a collaboration between the objects that you use, and this is true beyond painting. It’s true even in cooking, it’s true with clothes: if you have silk fabric, you’re going to make a different dress than if your fabric is linen. It’s the same with art: if you use a painter’s knife, you’re not going to make very precise little lines. And it’s the same thing in music: if you play the violin, it’s not like playing the piano. I’m very sensitive to the role of objects in our lives, to the importance of this collaboration. That’s how it is: the fact that I always use a knife explains why I’ve made flat color blocks. At first, I made them very naturally, as they came, instinctually. It’s like with children—you don’t teach them drawing; they paint naturally. We paint naturally, like we speak. This was incidentally the philosophy of that professor. She said: “Everyone can paint, just as everyone can speak.” This doesn’t mean that everyone is Picasso, but it’s a language.',\n",
       "   'role': 'EA'},\n",
       "  {'content': 'Your paintings don’t have titles. Why?', 'role': 'LA'},\n",
       "  {'content': 'I think titles are either too simple—if it’s a mountain, we say “Title: Mountain”—or they’re impossible. How to title an abstract painting? An abstract painting is like music, which doesn’t really call for a title—we say “sonata”—and abstraction is visual music. It’s the same, it has a meaning not destined to be put into words. When I was a professor of philosophy of art, I had students who told me: “This picture has no meaning,” and it was very hard to explain the abstract to them without nattering on or making things up. So, I would respond: “Look, when you listen to Beethoven, for example, you don’t say that it has no meaning, but you don’t explain it either.” The same goes for an abstract painting. It’s not made to be translated into words.',\n",
       "   'role': 'EA'},\n",
       "  {'content': 'I have in front of me a print of painting 38, from the catalog of an exhibition at Galerie Lelong—so no title, only “oil on canvas, 34.5 x 45 cm.” This is how you name your paintings. I myself can see many things here, but you—what do you see in this painting, which is one of your recent works?',\n",
       "   'role': 'LA'},\n",
       "  {'content': 'I see memories of landscapes, but not memories of specific landscapes, more like memories of a region. I lived for half a century just north of San Francisco, four kilometers from the ocean, and I saw the bay from my windows, so it involves an accumulation of experiences . . .',\n",
       "   'role': 'EA'},\n",
       "  {'content': 'A bit like Cézanne.', 'role': 'LA'},\n",
       "  {'content': 'Yes, yes! I mean, Cézanne could not escape the mountain. It’s not even a choice, the mountain is so extraordinary that it asserts itself. And I had a mountain in my window, but this mountain was surrounded by the bay on one side, by the ocean on the other. So, it’s not only the mountain, but also the whole “country,” the space, if you will; all this region that I saw constantly for years. And I suppose these are impressions that somehow record themselves in the cells of our brain, that come back to me whether I want them or not.',\n",
       "   'role': 'EA'},\n",
       "  {'content': 'It’s the remembrance of the experience of this Californian landscape that returns in your studio?',\n",
       "   'role': 'LA'},\n",
       "  {'content': 'It’s the remembrance of a life of impressions. It’s a cumulative experience, it’s not a precise place. Even when I mention the mountain, this mountain had infinite points of view: you went around it, you climbed it. It comes back to me most often the way I saw it from my windows. I can draw it with my eyes closed, I saw it so many times. But I was closer to the mountain than Cézanne was. Cézanne went to see it, whereas I had it in my bay window. I couldn’t see anything else. My paintings are compositions that reflect, that visually translate diverse experiences. I can’t tell you it’s this or that place. It’s a collage of these places.',\n",
       "   'role': 'EA'},\n",
       "  {'content': 'What’s very impressive in your approach as a painter—we’ll return to the importance of poetry and writing in your trajectory—is how emotion, the impression of a landscape and its translation, structure themselves in your thought. Your works are fairly small; you don’t make large paintings into which one might have the sense of being able to enter, physically. No, we enter, but by a kind of concentration . . .',\n",
       "   'role': 'LA'},\n",
       "  {'content': 'The formats are small, but the feeling isn’t small. That’s very important, and I don’t do it on purpose. I understood this intellectually when, at the start of my time in California, I saw some Japanese stamps, little postage stamps, and in these tiny little stamps there were such immense landscapes! This surprise has stayed with me. Now, this doesn’t mean I do it consciously—I don’t work as consciously as that. There is a truth in oneself. One must make do with that, and it comes out. Fortunately for me, my paintings had to be small because I always worked at home on a relatively small table. And I always had back pain as well. I have trouble moving in that way and I made very few big canvases, but the feeling I want to get across isn’t small. It’s landscapes and it’s the mystery of art. You control without controlling, and the vast landscape is there in these little paintings.',\n",
       "   'role': 'EA'},\n",
       "  {'content': 'Do you work every day?', 'role': 'LA'},\n",
       "  {'content': 'No, I’ve never done anything systematically. I think never in my life have I said that I was busy. When something is asked of me, I’m available. It’s a quality of character, not an effort. It makes it so that if I’m invited to dinner, I can go. When I can travel, I don’t say: “No, now I’ve got to work”; I travel. I had many odd jobs before becoming a professor. I took things as they came. Sometimes—I don’t know why, maybe I have something to write, or an event I need to participate in, or perhaps someone tells me: “You know, we need a piece for a magazine,” and I write a three-page text. The editor tells me: “No, it’s too short, you have to write more,” so I set myself to writing every day and I write a book called Paris, When it’s Naked. Chance plays an immense role in our lives. We think we’re directing things, but we’re also directed by what’s happening around us.',\n",
       "   'role': 'EA'},\n",
       "  {'content': 'Gaston Bachelard spoke of the porousness between dream and painting, literature, and art. Are you one of his followers?',\n",
       "   'role': 'LA'},\n",
       "  {'content': 'When I was a student at the Sorbonne, from 1949 to 1953, my two favorite professors were Souriau, the director of the thesis I never finished, and Bachelard. I took his courses, one of which made a big impression on me, on the notion of experience. Bachelard was very famous at the time. I think he’s a great philosopher that we don’t read enough. I’d compare him in importance to Heidegger because what I like in Heidegger is that, despite this feeling of constant abstraction, there is a sensitivity to the world, and Bachelard has this supreme quality—a sensitivity to the world—in plain evidence. Bachelard opened up ideas of poetry through his studies of childhood. He is a great poet-philosopher of poetry, very modern in this respect. There are two French thinkers, in my view, who directly or indirectly said amazing things: Bachelard and Malraux. Malraux is a great thinker of art, everyone knows this, and Bachelard just as much.',\n",
       "   'role': 'EA'},\n",
       "  {'content': 'Do you agree on the importance of the dream as a creative realm? Do you look to your dreams and to what might happen without you knowing?',\n",
       "   'role': 'LA'},\n",
       "  {'content': 'Without my knowing, maybe; surely; but I think everything is creation. Life is creation. We call it thinking because it’s more precise. It’s communication when one thinks. But it’s as if to say: night and day. Yes, we are living, everything is life and life takes certain forms, like electricity, like love; it makes you hot, it makes you cold, you love a tree; it comes from the same thing. And thought, poetry, even the act of walking—for me everything holds together. There is no break. We’re in a continuum. That’s it, the universe: a continuum in every direction, even the empty spaces are not empty; and this is what Bachelard says, he had this feeling of the cosmic where everything holds together.\\nLogical differences have been created for instruction. That is to say, in order to teach, one makes categories, as Aristotle made categories. But the unfortunate thing is that we’ve taken these categories to be absolute, separated, when in fact they are not separated, and Bachelard is in this continuum as if inside of a sphere. You go in all directions simultaneously and you choose. For instance, I see this glass of water, but the whole room is present, this glass isn’t in a void. I speak of the glass, but it doesn’t exist alone. Nothing exists alone; everything is connected. It’s inside connections, it seems to me, that we function.',\n",
       "   'role': 'EA'},\n",
       "  {'content': 'And inside these fields of sensation, how do you choose your mode of expression? Why, from time to time, is it poetry? Why, at other moments, is it literature or philosophy? And why could it be painting?',\n",
       "   'role': 'LA'},\n",
       "  {'content': 'I began as an undergraduate in literature and philosophy, and Gabriel Bounoure (he’s known among certain poets in France; he wrote introductions for Max Jacob and Henri Michaux, among others) was my professor, because he had founded a department, a literary institute, in Beirut, for the “license” [like a bachelor’s degree.–Ed.]. It didn’t exist before that; there was the Jesuit university in Beirut where all subjects were taught except literature and philosophy. And Gabriel Bounoure, who was sent there by the ministry of foreign affairs, had come as superintendent of French language education in Syria and Lebanon. In 1944, when Lebanon became independent, his position came to an end, and he thought: I must start an institute. Because he was in love with poetry, that’s what interested him most, and he didn’t want a Catholic institution to teach literature and philosophy, he wanted it to stay totally independent. And then of course after, when he left, they added these two missing subjects to the curriculum at the Université Saint-Joseph.\\nGabriel Bonoure taught poetry classes all the time, for the love of it, for pleasure, without looking at the time. For instance, during the Easter vacation, he arrived at two in the afternoon; at five the class was still going, and no one left. He talked, starting with Gérard de Nerval; he read Les Chimères, gave commentary, asked questions of the students, and continued with Baudelaire, always Baudelaire. We were being enlightened; we had known nothing and suddenly we read these texts that opened a world to us. So, we were practically electrified by philosophy and poetry. There were twelve pupils to begin with, and the school grew bit by bit. You realize, to have the luck to meet someone like this, who spoke of poetry for hours and who listened, who asked us questions, who knew each of us personally.\\nUnder his influence I began to write poetry, and I wrote a poem at around twenty years old, entitled “The Book of the Sea,” because at the time Beirut was a small town—you saw the sea from everywhere. I never published this poem, and then later, when I left for America, I was facing a new language. It never even occurred to me that I didn’t know English. It took me a semester and then I myself was teaching in English. And that was when I became a painter.\\nDuring the Vietnam War, the importance of poetry in the antiwar movement in America was tremendous. American poets today never talk about it; they’ve put aside this period which was a great awakening of poetry. There’s a spirit of rebellion in the pioneers of that new poetry like Allen Ginsberg. He was a complete poet, at once lyrical and political. He takes it further; for him, poetry speaks to the entire American culture and the lived reality of Americans. One day, spontaneously, I began to militate against the war. I was horrified by the images of the Vietnam War. I think at the time there was less censorship of images of war. Later, when governments saw the importance of images to public opinion, they became cautious. But at the time the war was broadcast live, and I saw with my own eyes American soldiers with flamethrowers burning Vietnamese villages. You couldn’t . . . you couldn’t not look. The entire country reacted to these images because they went beyond political analyses. When you see a real war broadcast live, every day, it becomes unbearable. You don’t think any more about the reasons for or against. That’s when I wrote my first poems, and since they were accepted very quickly, I felt like I fit in with American poetry.',\n",
       "   'role': 'EA'},\n",
       "  {'content': 'You’ve published poetry in different languages and especially in English, which is how the world first came to know your poetic work, in America first. I’m going to read a small excerpt of a book entitled There, translated from the American. Here is what you write, Etel:\\nListen, listen if you care (or if you don’t), do not mistake wine for food, do experience fear outside your mother’s womb, remember with your guts, speak from my own heart, extricate yourself, if you can, from my rage.\\nThere, along the white marble climbing toward heaven and through a sky darkened with airplanes, listen, there’s noise, the gates to nothingness are open while you struggle, and stutter, and I speak with no voice.',\n",
       "   'role': 'LA'},\n",
       "  {'content': 'Yes . . . . What can I tell you?', 'role': 'EA'},\n",
       "  {'content': 'No, I only wanted us to listen to what you write.\\nYour poems are often fragmentary. They are manifestly products of association. They can speak of geographical continents very far from one another, and they also speak a lot about war, not only the Vietnam war but also in Lebanon.',\n",
       "   'role': 'LA'},\n",
       "  {'content': 'Yes, necessarily, because I’ve always thought one is traversed by what one lives through. I’m asked sometimes: “Why is there so much history of contemporary war in your poetry?” I’ve answered: “Because it’s not me that writes it, it’s history that writes it.” I have often wanted to think of something besides war, but always another conflict arose. I was born into a family where there were only three of us. My father was an officer of the empire, classmate of Mustafa Kemal Ataturk, the founder and first president of the Republic of Turkey. My father had been at the battle of the Dardanelles. At thirty-eight years old his career was over; the Ottoman Empire had disappeared. So, he was a man whose life was somewhat tragic, by reason of history. My mother was much less cultivated than my father, but even more viscerally attached to her Greek world in Turkey. She saw Smyrna burn when she was only twenty-four years old. She thought only of Smyrna. She had stayed there, mentally. When I would go walking along the corniche in Beirut, along the waterfront, I would see huge clouds on the horizon and I would say to my mother, to make her happy: “Is that Smyrna?” I heard talk only of Smyrna. Her friends came to the house and always repeated: “The grapes were better in Smyrna; the fish was better in Smyrna . . . ” She was possessed. She was exiled from her daily life, and she lived in the absence of this city. Her family dispersed: one brother in Salonika, one brother in Cyprus, one brother in Alexandria, a sister in Italy. You see, wars are explosions of families, of lives; and I lived in this repetition of history at home, these stories of interminable war.\\nWe spoke Greek and Turkish in our home. My father didn’t speak Greek. But my mother spoke Greek to me very naturally, because it was really the language she knew best. She also spoke a bit of French, and she had learned Turkish, since she was born there. And there were two religions at home: my father was Muslim; my mother was Greek Orthodox. I think I’ve been very lucky to have grown up considering the world to be inhabited by different kinds of people. It’s an advantage, on account of it being natural. It wasn’t something you had to figure out. You even expected that suddenly someone might show up who wasn’t quite the same as what you already knew. The Near East in those days was a stopping-off point. It was the end of an empire, with a changing population, and school was French. Under the mandate, we were forbidden to speak Arabic, even at home. I learned it later, haphazardly, in the street. I regret that, by the way. I would have liked to know it better. Anyway, I went to the convent school. The education was very strict. These were very good schools, but they were also a bit narrow. They would say: “Poor little Etel, everything is as it should be, but her father is Muslim.” We were told that Muslims couldn’t get into Heaven . . . . I learned to think critically, because at home I heard another side of the story. It was the Greek world; things were more accepted. I was an only child. We would go to Damascus to visit my aunt. There it was still the Ottoman empire. It was another world, the end of a world. And even in Beirut, there was the cosmopolitan neighborhood where I lived, which was no longer solely Christian.\\nWe never went to the other side of the boulevard because we had nothing to do there. There were two or three tramway lines, but we got around on foot most of the time. There were three taxis; you hailed them by pointing a finger. I consider all of that to be a poetic world. Because it was innocent. Everything was in a minor mode. There was no pollution. We were coming out of the Great War; it had taken years. We didn’t know where the world was headed; we didn’t think it was headed anywhere. We lived each day by itself. We knew only the present. It was an idyllic world, and I was of the first generation of girls to be allowed to swim. I lived through the liberation of women: a very interesting question, because it’s the result of little jumps, as if you were jumping a few meters at a time. At first, there were ten or so little girls who went swimming. Not more than that. And then suddenly, you saw women eating ice cream in the ice cream shop. That’s how it took place. In little stages.',\n",
       "   'role': 'EA'},\n",
       "  {'content': 'What’s amazing with you, Etel, is your feeling that the beauty of the world is real. Moreover, you’ve celebrated it your whole life, whether it be in poetry, painting, or calligraphy. And I think when you were little, your parents put you in the garden and you spoke to the flowers.',\n",
       "   'role': 'LA'},\n",
       "  {'content': 'Yes. At one time, my parents rented a large house, and I remember my mother telling me: “You sit on the stool and don’t move.” So, I was sitting there like that, for an hour or two, and since I had no brother or sister, I would speak aloud, I would communicate with the garden. There was also a cat, who belonged to my mother, not to me. The house was her house. For Greek wives, the house belongs to them. It doesn’t belong to the family; it belongs to the wife. Fathers bleed themselves dry to buy a house for their daughter. If they can’t buy a house, they buy an armoire. You see? The daughter must be given a foundation. It’s more than an object. Therefore, we were in my mother’s house. That really affected me, and it made it so that I spent my life outside, in cafés and traveling. I was never really at home indoors. This is how I was raised: “Don’t move, don’t touch. We’re cleaning the house.”',\n",
       "   'role': 'EA'},\n",
       "  {'content': 'What did you say to the flowers, then?', 'role': 'LA'},\n",
       "  {'content': 'I believe the nuns spoke to us about miracles. The education we had was a kind of irrationalism—sometimes charming, sometimes terrifying. We spoke of things that were completely invisible. Paradise, saints, sins; it easily prepares you for a world that’s a bit surrealist. And I think I was there, dreaming that I worked miracles. What miracles? For example, I would give agency to flowers. Suddenly they could reply to me. These days, being an only child is almost the rule. But in my generation, at school, I was the only girl with neither brother nor sister. So, I watched the world. The world kept me company. I observed. I wanted to step on my own shadow. When returning home from the beach, the sun was such that my shadow was in front of me, and I wanted to step on it. This could be a poem in itself. We don’t need to add a thing. To observe the world is poetry. The world is exciting and when you don’t have lots of other children around, you speak to yourself, you look around more.\\nThen the Second World War began. My parents were broke, and at sixteen, I went to work in an office while studying for my two baccalaureate exams, at school or in private tutorial. And there, work was a huge discovery. Especially at a time when women were still at home. The world of work is really a second birth. In the first place, you meet people who aren’t friends of your parents. You’re a girl, and you speak to boys. You earn money, even if my mom took it from me, because girls gave their money to the family. You realize, all of this during my own lifetime. It’s an extraordinary reservoir of poetry. For Beirut, the war was a boom, since wars make fortunes for many people. Everything changes; everything moves very fast. I followed the war with my cohort at the office. We had tacked up a map of the battle of Stalingrad on a large wall. I was even at a press conference that De Gaulle gave in Beirut, and I recorded the conference in shorthand with the secretary of the chief of staff. I was busy, but in a positive way; it was much more interesting than school or home. I had the sense of being active in history, of living it. And then there was the war with Nasser in 1956; the coup d’état in Beirut in 1958; the war of 1967; the civil war from 1975 to 1990. For fifteen years we spent our time listening to the war on the radio, so how could I not think about it?\\nI’m like an antenna that picked up what was happening in the world, and suddenly I didn’t want to write anymore. I had nothing to say. Our life is determined by big decisions, but on the inside life isn’t determined. You can do something this afternoon that you didn’t plan to do. You can meet someone who will suddenly change your life without your having known it a few minutes prior. So, chance—whether it be inside a painting or in daily life or in artistic decisions—chance collaborates with us.',\n",
       "   'role': 'EA'}]}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTTrainer\n",
    "import os\n",
    "import torch\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import (\n",
    "    BitsAndBytesConfig,\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorWithPadding,\n",
    "    DefaultDataCollator,\n",
    ")\n",
    "from trl import SFTTrainer, setup_chat_format, SFTConfig\n",
    "from peft import LoraConfig\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Configure LoRA parameters\n",
    "rank_dimension = 6\n",
    "lora_alpha = 8\n",
    "lora_dropout = 0.05\n",
    "run_name = \"etel_adnan\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    r=rank_dimension,  # Rank dimension - typically between 4-32\n",
    "    lora_alpha=lora_alpha,  # LoRA scaling factor - typically 2x rank\n",
    "    lora_dropout=lora_dropout,  # Dropout probability for LoRA layers\n",
    "    bias=\"none\",  # Bias type for LoRA. the corresponding biases will be updated during training.\n",
    "    target_modules=\"all-linear\",  # Which modules to apply LoRA to\n",
    "    task_type=\"CAUSAL_LM\",  # Task type for model architecture\n",
    ")\n",
    "\n",
    "# Training configuration\n",
    "# Hyperparameters based on QLoRA paper recommendations\n",
    "args = SFTConfig(\n",
    "    output_dir=run_name,  # Directory to save model checkpoints\n",
    "    num_train_epochs=1,  # Number of training epochs\n",
    "    per_device_train_batch_size=1,  # Batch size per GPU\n",
    "    gradient_accumulation_steps=2,  # Accumulate gradients for larger effective batch\n",
    "    gradient_checkpointing=True,  # Trade compute for memory savings\n",
    "    optim=\"adamw_torch_fused\",  # Use fused AdamW for efficiency\n",
    "    learning_rate=2e-4,  # Learning rate (QLoRA paper)\n",
    "    max_grad_norm=0.3,  # Gradient clipping threshold\n",
    "    warmup_ratio=0.03,  # Portion of steps for warmup\n",
    "    lr_scheduler_type=\"constant\",  # Keep learning rate constant after warmup\n",
    "    logging_steps=10,  # Log metrics every N steps\n",
    "    save_strategy=\"epoch\",  # Save checkpoint every epoch\n",
    "    bf16=True,  # Use bfloat16 precision\n",
    "    push_to_hub=False,  # Don't push to HuggingFace Hub\n",
    "    report_to=\"none\",  # Disable external logging\n",
    "    max_seq_length=4096,\n",
    "    packing=False, # Don't concatenate multiple sequences to meet max_seq_length\n",
    ")\n",
    "\n",
    "# Create SFTTrainer with LoRA configuration\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    data_collator=DataCollatorWithPadding(tokenizer=tokenizer),\n",
    "    train_dataset=ds,\n",
    "    processing_class=tokenizer,\n",
    "    peft_config=peft_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "SFTTrainer._prepare_dataset() missing 8 required positional arguments: 'dataset', 'processing_class', 'packing', 'dataset_text_field', 'max_seq_length', 'formatting_func', 'num_of_sequences', and 'chars_per_token'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# trainer._prepare_non_packed_dataloader(ds)\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_prepare_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: SFTTrainer._prepare_dataset() missing 8 required positional arguments: 'dataset', 'processing_class', 'packing', 'dataset_text_field', 'max_seq_length', 'formatting_func', 'num_of_sequences', and 'chars_per_token'"
     ]
    }
   ],
   "source": [
    "# trainer._prepare_non_packed_dataloader(ds)\n",
    "trainer._prepare_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([    1, 49152, 49154,  ...,  1083,    30,     2]), 'attention_mask': tensor([1, 1, 1,  ..., 1, 1, 1])}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_collator({\"input_ids\": data[0]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_data = [tokenizer(chunk, return_tensors=\"pt\") for chunk in final_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "DataCollatorMixin.__call__() got an unexpected keyword argument 'batch_size'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[190], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m collated_data \u001b[38;5;241m=\u001b[39m \u001b[43mdata_collator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokenized_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: DataCollatorMixin.__call__() got an unexpected keyword argument 'batch_size'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "collated_data = data_collator(tokenized_data, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add new tokens to the embedding matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
