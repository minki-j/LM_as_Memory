{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "model_name = \"HuggingFaceTB/SmolLM2-135M\"\n",
    "dataset_path = \"HuggingFaceTB/smoltalk\"\n",
    "dataset_name = \"everyday-conversations\"\n",
    "ebook_file_path = \"\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess the book into a txt file that has role and content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ebooklib import epub\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "def read_epub(file_path):\n",
    "    book = epub.read_epub(file_path)\n",
    "    text_content = []\n",
    "\n",
    "    for item in book.get_items():\n",
    "        if isinstance(item, epub.EpubHtml):\n",
    "            soup = BeautifulSoup(item.content, \"html.parser\")\n",
    "            text_content.append(soup.get_text())\n",
    "\n",
    "    return \"\\n\".join(text_content)\n",
    "\n",
    "\n",
    "if ebook_file_path != \"\":\n",
    "    text = read_epub(ebook_file_path)\n",
    "    with open(\"../datasets/etel_adnan.txt\", \"w\") as f:\n",
    "        f.write(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open the txt file and print the first 500 characters\n",
    "with open(\"../datasets/etel_adnan.txt\", \"r\") as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the book into chapters\n",
    "candidates = text.split(\"\\n\\n\\n\\n\\n\\n\")\n",
    "final = []\n",
    "for candidate in candidates:\n",
    "    candidate = candidate.strip()\n",
    "    if len(candidate) > 1000 and candidate[0] in [\n",
    "        \"1\",\n",
    "        \"2\",\n",
    "        \"3\",\n",
    "        \"4\",\n",
    "        \"5\",\n",
    "        \"6\",\n",
    "        \"7\",\n",
    "        \"8\",\n",
    "        \"9\",\n",
    "        \"10\",\n",
    "        \"11\",\n",
    "    ]:\n",
    "        final.append(\"\\n\\n\".join(candidate.split(\"\\n\\n\")[1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Abbreviate the names for the first two lines which is not abbreviated\n",
    "final[0] = final[0].replace(\"LAURE ADLER: \", \"LA: \").replace(\"ETEL ADNAN: \", \"EA: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_conversation(text):\n",
    "    result = []\n",
    "    chunks = text.split(\"LA: \")[1:]  # Skip empty first chunk\n",
    "\n",
    "    for chunk in chunks:\n",
    "        if \"EA: \" in chunk:\n",
    "            la_text, ea_chunk = chunk.split(\"EA: \")\n",
    "            result.append({\"role\": \"LA\", \"content\": la_text.strip()})\n",
    "            result.append({\"role\":\"EA\", \"content\": ea_chunk.strip()})\n",
    "        else:\n",
    "            result.append({\"LA\": chunk.strip()})\n",
    "\n",
    "    return result\n",
    "\n",
    "role_content_templated = []\n",
    "for chapter in final:\n",
    "    result = parse_conversation(chapter)\n",
    "    role_content_templated.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'LA',\n",
       "  'content': 'Etel, you are a writer, a poet, an artist; you were born in Lebanon. In which language were you brought up?'},\n",
       " {'role': 'EA',\n",
       "  'content': 'I’m a bit of a particular case, especially for the time. My mother was Greek, from Smyrna (now Izmir), which is to say from Turkey, and my father was born in Damascus; he was also an officer of the Ottoman empire, so the common language between them was Turkish. We spoke Turkish in Beirut, at home, but my mother spoke to me in Greek, naturally. I grew up this way until the age of twenty, until twenty-four even, speaking Greek and Turkish, and French, because at the time the schools were strictly French speaking; Arabic wasn’t taught. I “caught”—as the saying goes—my Arabic in the street and with other children. So, I grew up in four languages.'}]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "role_content_templated[0][:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"../datasets/etel_adnan.json\", \"w\") as f:\n",
    "    json.dump(role_content_templated, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup tokenizer for chat template and special tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# special strings for role that will be added to tokenizer vocabulary\n",
    "\n",
    "role_A = \"#29njkn(dkj38$%nkjn#\" #Laure Adler\n",
    "role_B = \"#foi*Ewoh!@oih(&idl#\" #Etel Adnan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add chat template to tokenizer\n",
    "\n",
    "tokenizer.chat_template = \"{% for message in messages %}{% if (message['role'] == 'system') %}{{'<|im_start|>system<|im_sep|>' + message['content'] + '<|im_end|>'}}{% elif (message['role'] == 'LA') %}{{'<|im_start|>#29njkn(dkj38$%nkjn#<|im_sep|>' + message['content'] + '<|im_end|><|im_start|>#foi*Ewoh!@oih(&idl#<|im_sep|>'}}{% elif (message['role'] == 'EA') %}{{message['content'] + '<|im_end|>'}}{% endif %}{% endfor %}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49152"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = tokenizer.add_special_tokens(\n",
    "    {\n",
    "        \"additional_special_tokens\": tokenizer.additional_special_tokens\n",
    "        + [role_A, role_B, \"<|im_sep|>\"]\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49155"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bos_token': '<|endoftext|>',\n",
       " 'eos_token': '<|endoftext|>',\n",
       " 'unk_token': '<|endoftext|>',\n",
       " 'additional_special_tokens': ['<|endoftext|>',\n",
       "  '<|im_start|>',\n",
       "  '<|im_end|>',\n",
       "  '<repo_name>',\n",
       "  '<reponame>',\n",
       "  '<file_sep>',\n",
       "  '<filename>',\n",
       "  '<gh_stars>',\n",
       "  '<issue_start>',\n",
       "  '<issue_comment>',\n",
       "  '<issue_closed>',\n",
       "  '<jupyter_start>',\n",
       "  '<jupyter_text>',\n",
       "  '<jupyter_code>',\n",
       "  '<jupyter_output>',\n",
       "  '<jupyter_script>',\n",
       "  '<empty_output>',\n",
       "  '#29njkn(dkj38$%nkjn#',\n",
       "  '#foi*Ewoh!@oih(&idl#',\n",
       "  '<|im_sep|>']}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.special_tokens_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./tokenizer_config.json',\n",
       " './special_tokens_map.json',\n",
       " './vocab.json',\n",
       " './merges.txt',\n",
       " './added_tokens.json',\n",
       " './tokenizer.json')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.save_pretrained(\"./\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"./\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49155"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [49152], 'attention_mask': [1]}\n",
      "{'input_ids': [49153], 'attention_mask': [1]}\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer(role_A))\n",
    "print(tokenizer(role_B))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply tokenizer and split at max_seq_length\n",
    "\n",
    "We need to split at utterance level, meaning that the chunk should be split at the end of the interviewer finished talking. We also need to keep each chunk with the same token length. \n",
    "\n",
    "So we are going to first tokenize and split using max_seq_length, and find the closest utterance end. Then we are going to add padding tokens to make it max_seq_length. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"../datasets/etel_adnan.json\", \"r\") as f:\n",
    "    data = f.read()\n",
    "    data = json.loads(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 : <|im_start|>\n",
      "49152 : #29njkn(dkj38$%nkjn#\n",
      "49154 : <|im_sep|>\n",
      "6004 : hi\n",
      "2 : <|im_end|>\n",
      "1 : <|im_start|>\n",
      "49153 : #foi*Ewoh!@oih(&idl#\n",
      "49154 : <|im_sep|>\n",
      "6004 : hi\n",
      "2 : <|im_end|>\n"
     ]
    }
   ],
   "source": [
    "# test tokenizer apply_chat_template method \n",
    "\n",
    "tokens =tokenizer.apply_chat_template(\n",
    "    [\n",
    "        {\n",
    "            \"role\": \"LA\",\n",
    "            \"content\": \"hi\",\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"EA\",\n",
    "            \"content\": \"hi\",\n",
    "        },\n",
    "    ],\n",
    "    tokenize=True\n",
    ")\n",
    "\n",
    "for token in tokens:\n",
    "    print(token ,\":\", tokenizer.decode(token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8200 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "# apply to the whole chapters\n",
    "\n",
    "chat_templated_tokens = [\n",
    "    tokenizer.apply_chat_template(chapter, tokenize=True) for chapter in data\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7665\n",
      "7227\n",
      "4605\n",
      "5466\n",
      "4550\n",
      "1441\n",
      "2686\n",
      "3550\n",
      "3205\n",
      "8200\n",
      "4388\n"
     ]
    }
   ],
   "source": [
    "for chapter in chat_templated_tokens:\n",
    "    print(len(chapter), len([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividing with padding with fixed maximum length. split at utterance level\n",
    "\n",
    "max_seq_length = 2048\n",
    "split_token_sequence = [1, 49152]  # tokens for <|im_start|> and 29njkn(dkj38$%nkjn#\n",
    "pad_sequence = tokenizer(\"<|endoftext|>\")[\"input_ids\"][0]\n",
    "\n",
    "\n",
    "def find_last_sequence(lst, sequence):\n",
    "    for i in range(len(lst) - len(sequence), -1, -1):  # Search backwards\n",
    "        if lst[i : i + len(sequence)] == sequence:\n",
    "            return i\n",
    "    return -1\n",
    "\n",
    "\n",
    "split_padded_tokens = []\n",
    "for i, chapter in enumerate(chat_templated_tokens):\n",
    "    if len(chapter) < max_seq_length:\n",
    "        chapter = [pad_sequence for _ in range(max_seq_length - len(chapter))] + chapter\n",
    "\n",
    "        split_padded_tokens.append(\n",
    "            {\n",
    "                \"input_ids\": chapter,\n",
    "                \"attention_mask\": [0.0 for _ in range(max_seq_length - len(chapter))]\n",
    "                + [1.0 for _ in range(len(chapter))],\n",
    "            }\n",
    "        )\n",
    "    else:\n",
    "\n",
    "        while True:\n",
    "            split_with_max_seq_len = chapter[:max_seq_length]\n",
    "            last_index = find_last_sequence(\n",
    "                split_with_max_seq_len, split_token_sequence\n",
    "            )\n",
    "\n",
    "            if last_index == 0:\n",
    "                # If utterance ex\n",
    "                split_at_utterance_level = chapter[:max_seq_length]\n",
    "                split_padded_tokens.append(\n",
    "                    {\n",
    "                        \"input_ids\": split_at_utterance_level,\n",
    "                        \"attention_mask\": [1.0 for _ in range(max_seq_length)],\n",
    "                    }\n",
    "                )\n",
    "            else:\n",
    "                split_at_utterance_level = chapter[:last_index]\n",
    "                split_at_utterance_level = [\n",
    "                    pad_sequence for _ in range(max_seq_length - last_index)\n",
    "                ] + split_at_utterance_level\n",
    "                split_padded_tokens.append(\n",
    "                    {\n",
    "                        \"input_ids\": split_at_utterance_level,\n",
    "                        \"attention_mask\": [\n",
    "                            0.0 for _ in range(max_seq_length - last_index)\n",
    "                        ]\n",
    "                        + [1.0 for _ in range(last_index)],\n",
    "                    }\n",
    "                )\n",
    "            chapter = chapter[last_index:]\n",
    "            if len(chapter) < max_seq_length:\n",
    "                # print(\"last_chunk\")\n",
    "                split_padded_tokens.append(\n",
    "                    {\n",
    "                        \"input_ids\": [\n",
    "                            pad_sequence for _ in range(max_seq_length - len(chapter))\n",
    "                        ]\n",
    "                        + chapter,\n",
    "                        \"attention_mask\": [\n",
    "                            0.0 for _ in range(max_seq_length - len(chapter))\n",
    "                        ]\n",
    "                        + [1.0 for _ in range(len(chapter))],\n",
    "                    }\n",
    "                )\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "’s not made to be translated into words.<|im_end|>\n",
      "tions, it seems to me, that we function.<|im_end|>\n",
      "im_sep|>Yes . . . . What can I tell you?<|im_end|>\n",
      "ve tree on the balcony. It’s a good day.<|im_end|>\n"
     ]
    }
   ],
   "source": [
    "# check if it was splited at the end of Etel's utterance\n",
    "\n",
    "print(tokenizer.decode(split_padded_tokens[0][\"input_ids\"])[-50:])\n",
    "print(tokenizer.decode(split_padded_tokens[1][\"input_ids\"])[-50:])\n",
    "print(tokenizer.decode(split_padded_tokens[2][\"input_ids\"])[-50:])\n",
    "print(tokenizer.decode(split_padded_tokens[-1][\"input_ids\"])[-50:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(split_padded_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{2048}\n"
     ]
    }
   ],
   "source": [
    "print(set([len(chunk[\"input_ids\"]) for chunk in split_padded_tokens]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict = {\"input_ids\": [], \"attention_mask\": [], \"labels\": []}\n",
    "for chunk in split_padded_tokens:\n",
    "    data_dict[\"input_ids\"].append(chunk[\"input_ids\"])\n",
    "    data_dict[\"attention_mask\"].append([bool(x) for x in chunk[\"attention_mask\"]])\n",
    "    data_dict[\"labels\"].append(chunk[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, Dataset\n",
    "\n",
    "ds = Dataset.from_dict(data_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(27, 2048)\n",
      "(27, 2048)\n",
      "(27, 2048)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "print(np.array(ds[\"input_ids\"]).shape)\n",
    "print(np.array(ds[\"attention_mask\"]).shape)\n",
    "print(np.array(ds[\"labels\"]).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bool"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(ds[\"attention_mask\"][0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7fbfa01516f4ad7af4598e6ffe2f5ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/27 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ds.save_to_disk(\"../datasets/etel_adnan_dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 27\n",
       "})"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_from_disk\n",
    "\n",
    "ds = load_from_disk(\"../datasets/etel_adnan_dataset\")\n",
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To upload to Modal volume\n",
    "\n",
    "modal volume rm -r lm-as-memory dataset                            \n",
    "modal volume put lm-as-memory ./datasets/etel_adnan_dataset dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(27, 2048)\n",
      "(27, 2048)\n",
      "(27, 2048)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "print(np.array(ds[\"input_ids\"]).shape)\n",
    "print(np.array(ds[\"attention_mask\"]).shape)\n",
    "print(np.array(ds[\"labels\"]).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add new tokens to the embedding matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
